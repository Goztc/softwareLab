# RAGæµæ°´çº¿ - åŸºäºLangChainå®ç°
# Author: AI Assistant
# Version: 3.0 - åŸºäºRAG-Tongyi.pyä¼˜åŒ–

import os
from pathlib import Path
from typing import Optional, List

# LangChain imports - ä½¿ç”¨æœ€æ–°çš„å¯¼å…¥æ–¹å¼
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    TextLoader, UnstructuredMarkdownLoader, PyPDFLoader, Docx2txtLoader, DirectoryLoader
)
from langchain_core.documents import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatTongyi

# æœ¬åœ°å¯¼å…¥
from config import QWEN_API_KEY, QWEN_MODEL_NAME, EMBEDDING_MODEL_NAME

class RAGPipeline:
    """RAGæµæ°´çº¿ç±» - åŸºäºLangChainå®ç°ï¼Œå‚è€ƒRAG-Tongyi.pyä¼˜åŒ–"""
    
    def __init__(self):
        """åˆå§‹åŒ–RAGæµæ°´çº¿"""
        print("ğŸ”§ åˆå§‹åŒ–RAGæµæ°´çº¿...")
        
        # è®¾ç½®é€šä¹‰åƒé—®APIå¯†é’¥
        os.environ["DASHSCOPE_API_KEY"] = QWEN_API_KEY
        
        # åˆå§‹åŒ–é€šä¹‰åƒé—®æ¨¡å‹ - å‚è€ƒRAG-Tongyi.pyçš„å®ç°
        self.llm = ChatTongyi(
            model=QWEN_MODEL_NAME,  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
            temperature=0.1,  # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
            top_p=0.9,  # æ§åˆ¶è¯æ±‡é€‰æ‹©çš„å¤šæ ·æ€§
        )
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,  # å¢åŠ é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        
        # åˆå§‹åŒ–æç¤ºæ¨¡æ¿ - å‚è€ƒRAG-Tongyi.pyçš„è®¾è®¡
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚è¯·ç¡®ä¿å›ç­”å‡†ç¡®ã€ç®€æ´ã€æœ‰å¸®åŠ©ã€‚
å¦‚æœæä¾›çš„æ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦‚å®è¯´æ˜ã€‚

ç›¸å…³æ–‡æ¡£:
{context}

é—®é¢˜: {question}

å›ç­”:"""
        )
        
        # åˆå§‹åŒ–å…¶ä»–å±æ€§
        self.conversation_histories = {}
        
        print("âœ… RAGæµæ°´çº¿åˆå§‹åŒ–æˆåŠŸ")
    
    def _load_documents_from_path(self, document_paths) -> List[Document]:
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ–‡æ¡£ï¼Œæ”¯æŒå¤šä¸ªè·¯å¾„"""
        # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(document_paths, str):
            document_paths = [document_paths]
        
        all_documents = []
        
        for document_path in document_paths:
            if not os.path.exists(document_path):
                print(f"âš ï¸  è­¦å‘Šï¼šæ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡: {document_path}")
                continue
            
            documents = []
            
            if os.path.isfile(document_path):
                # å•ä¸ªæ–‡ä»¶
                documents.extend(self._load_single_file(document_path))
                print(f"ğŸ“„ åŠ è½½æ–‡ä»¶: {document_path}")
            elif os.path.isdir(document_path):
                # ç›®å½• - é€’å½’éå†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹
                print(f"ğŸ“ åŠ è½½ç›®å½•: {document_path}")
                file_count = 0
                for file_path in Path(document_path).rglob("*"):
                    if file_path.is_file() and file_path.suffix.lower() in ['.txt', '.md', '.pdf', '.docx']:
                        file_documents = self._load_single_file(str(file_path))
                        documents.extend(file_documents)
                        if file_documents:  # åªæœ‰æˆåŠŸåŠ è½½çš„æ–‡ä»¶æ‰è®¡æ•°
                            file_count += 1
                            print(f"  ğŸ“„ å·²åŠ è½½: {file_path.name}")
                print(f"ğŸ“ ç›®å½• {document_path} å…±åŠ è½½ {file_count} ä¸ªæ–‡ä»¶")
            
            all_documents.extend(documents)
        
        print(f"ğŸ“š æ€»å…±åŠ è½½äº† {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        file_extension = Path(file_path).suffix.lower()
        
        try:
            if file_extension == '.txt':
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_extension == '.md':
                loader = UnstructuredMarkdownLoader(file_path)
            elif file_extension == '.pdf':
                loader = PyPDFLoader(file_path)
            elif file_extension in ['.docx', '.doc']:
                loader = Docx2txtLoader(file_path)
            else:
                # é»˜è®¤ä½œä¸ºæ–‡æœ¬æ–‡ä»¶å¤„ç†
                loader = TextLoader(file_path, encoding='utf-8')
            
            return loader.load()
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
            return []
    
    def query(self, question: str, document_path, top_k: int = 3) -> dict:
        """
        æŸ¥è¯¢æ–‡æ¡£å¹¶è¿”å›ç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document_path: æ–‡æ¡£è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€‚æ”¯æŒæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–æ··åˆï¼‰
            top_k: è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œæ¥æºæ–‡æ¡£çš„å­—å…¸
        """
        try:
            # åŠ è½½æ–‡æ¡£ - ç°åœ¨æ”¯æŒå¤šä¸ªè·¯å¾„
            documents = self._load_documents_from_path(document_path)
            if not documents:
                return {
                    "question": question,
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚",
                    "sources": []
                }
            
            print(f"ğŸ“Š å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºæŸ¥è¯¢")
            
            # åˆ†å‰²æ–‡æ¡£ - å‚è€ƒRAG-Tongyi.pyçš„å®ç°
            doc_splits = self.text_splitter.split_documents(documents)
            print(f"ğŸ”ª å·²å°†æ–‡æ¡£åˆ†å‰²ä¸º {len(doc_splits)} ä¸ªæ–‡æœ¬å—")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vectorstore = FAISS.from_documents(doc_splits, self.embeddings)
            print("ğŸ—„ï¸  å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
            
            # åˆ›å»ºæ£€ç´¢å™¨
            retriever = vectorstore.as_retriever(
                search_kwargs={"k": top_k}  # æ£€ç´¢å‰kä¸ªæœ€ç›¸å…³çš„å—
            )
            
            # åˆ›å»ºRetrievalQAé“¾ - å‚è€ƒRAG-Tongyi.pyçš„å®ç°
            retrieval_qa = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",  # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£æ”¾å…¥æç¤ºä¸­
                retriever=retriever,
                return_source_documents=True,  # åœ¨å“åº”ä¸­åŒ…å«æºæ–‡æ¡£
                chain_type_kwargs={"prompt": self.prompt_template}  # ä½¿ç”¨è‡ªå®šä¹‰æç¤º
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = retrieval_qa.invoke({"query": question})
            
            # æå–æºæ–‡æ¡£ä¿¡æ¯
            sources = []
            source_files = []
            for doc in result.get("source_documents", []):
                source_info = doc.metadata.get("source", "Unknown")
                if source_info not in source_files:
                    source_files.append(source_info)
                    sources.append({
                        "content": doc.page_content,
                        "source": source_info
                    })
            
            return {
                "question": question,
                "answer": result["result"],
                "sources": sources
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "question": question,
                "answer": f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": []
            }
    
    def chat(self, question: str, document_path, history: Optional[List] = None, conversation_id: str = "default") -> dict:
        """
        å¸¦å†å²å¯¹è¯çš„æŸ¥è¯¢ - çœŸæ­£çš„å¯¹è¯å¼RAGå®ç°
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document_path: æ–‡æ¡£è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€‚æ”¯æŒæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–æ··åˆï¼‰
            history: å¯¹è¯å†å²
            conversation_id: å¯¹è¯IDï¼Œç”¨äºæ ‡è¯†ä¸åŒçš„å¯¹è¯ä¼šè¯
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œæ¥æºæ–‡æ¡£çš„å­—å…¸
        """
        try:
            # è·å–æˆ–åˆå§‹åŒ–å¯¹è¯å†å²
            if conversation_id not in self.conversation_histories:
                self.conversation_histories[conversation_id] = []
            
            current_history = self.conversation_histories[conversation_id]
            
            # å¦‚æœä¼ å…¥äº†historyå‚æ•°ï¼Œä½¿ç”¨å®ƒæ¥æ›´æ–°å½“å‰å†å²
            if history is not None:
                current_history = history.copy()
                self.conversation_histories[conversation_id] = current_history
            
            # åŠ è½½æ–‡æ¡£
            documents = self._load_documents_from_path(document_path)
            if not documents:
                return {
                    "question": question,
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚",
                    "response": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚",
                    "sources": [],
                    "updated_history": current_history,
                    "conversation_id": conversation_id
                }
            
            print(f"ğŸ“Š å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºå¯¹è¯æŸ¥è¯¢")
            
            # åˆ†å‰²æ–‡æ¡£
            doc_splits = self.text_splitter.split_documents(documents)
            print(f"ğŸ”ª å·²å°†æ–‡æ¡£åˆ†å‰²ä¸º {len(doc_splits)} ä¸ªæ–‡æœ¬å—")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vectorstore = FAISS.from_documents(doc_splits, self.embeddings)
            print("ğŸ—„ï¸  å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
            
            # åˆ›å»ºæ£€ç´¢å™¨
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = retriever.get_relevant_documents(question)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
            
            # æ„å»ºåŒ…å«å†å²å¯¹è¯çš„æç¤º
            conversation_context = ""
            if current_history:
                print(f"ğŸ’¬ ä½¿ç”¨ {len(current_history)} è½®å†å²å¯¹è¯")
                conversation_context = "\nä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯å†å²:\n"
                for i, exchange in enumerate(current_history[-5:], 1):  # åªä½¿ç”¨æœ€è¿‘5è½®å¯¹è¯
                    conversation_context += f"ç¬¬{i}è½®å¯¹è¯:\n"
                    conversation_context += f"ç”¨æˆ·: {exchange.get('question', '')}\n"
                    conversation_context += f"åŠ©æ‰‹: {exchange.get('answer', '')}\n\n"
                conversation_context += "---\n"
            
            # åˆ›å»ºå¢å¼ºçš„æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«å†å²å¯¹è¯
            enhanced_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªåŸºäºæ–‡æ¡£çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{conversation_context}

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

å½“å‰ç”¨æˆ·é—®é¢˜: {question}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœé—®é¢˜æ¶‰åŠä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·ç»“åˆå†å²å¯¹è¯æ¥å›ç­”ã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦‚å®è¯´æ˜ã€‚

å›ç­”:"""
            
            # ç›´æ¥è°ƒç”¨LLMç”Ÿæˆå›ç­”
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå¸¦å†å²å¯¹è¯çš„å›ç­”...")
            response = self.llm.invoke(enhanced_prompt)
            
            # æå–å›ç­”å†…å®¹
            if hasattr(response, 'content'):
                answer = response.content
            else:
                answer = str(response)
            
            # æå–æºæ–‡æ¡£ä¿¡æ¯
            sources = []
            source_files = []
            for doc in retrieved_docs:
                source_info = doc.metadata.get("source", "Unknown")
                if source_info not in source_files:
                    source_files.append(source_info)
                    sources.append({
                        "content": doc.page_content,
                        "source": source_info
                    })
            
            # æ·»åŠ å½“å‰å¯¹è¯åˆ°å†å²
            current_exchange = {
                "question": question,
                "answer": answer,
                "sources": sources,
                "timestamp": __import__('datetime').datetime.now().isoformat()
            }
            
            current_history.append(current_exchange)
            self.conversation_histories[conversation_id] = current_history
            
            print(f"âœ… å¯¹è¯å›ç­”ç”ŸæˆæˆåŠŸï¼Œå†å²è®°å½•å·²æ›´æ–° (å…±{len(current_history)}è½®)")
            
            return {
                "question": question,
                "answer": answer,
                "response": answer,  # ä¸ºäº†å…¼å®¹æ€§
                "sources": sources,
                "updated_history": current_history.copy(),
                "conversation_id": conversation_id
            }
            
        except Exception as e:
            print(f"âŒ å¯¹è¯æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "question": question,
                "answer": f"å¯¹è¯æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "response": f"å¯¹è¯æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": [],
                "updated_history": current_history,
                "conversation_id": conversation_id
            }
    
    def search(self, query: str, document_path, top_k: int = 5) -> dict:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            document_path: æ–‡æ¡£è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€‚æ”¯æŒæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–æ··åˆï¼‰
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        try:
            # åŠ è½½æ–‡æ¡£
            documents = self._load_documents_from_path(document_path)
            if not documents:
                return {
                    "query": query,
                    "results": [],
                    "message": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                }
            
            # åˆ†å‰²æ–‡æ¡£
            doc_splits = self.text_splitter.split_documents(documents)
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vectorstore = FAISS.from_documents(doc_splits, self.embeddings)
            
            # æ‰§è¡Œæœç´¢
            retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
            docs = retriever.get_relevant_documents(query)
            
            results = []
            for i, doc in enumerate(docs):
                # å¦‚æœæ–‡æ¡£æœ‰scoreå±æ€§åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨åŸºäºæ’åçš„é»˜è®¤åˆ†æ•°
                score = getattr(doc, 'score', None)
                if score is None:
                    # åŸºäºæ’åç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿåˆ†æ•°ï¼Œæ’åè¶Šé å‰åˆ†æ•°è¶Šé«˜
                    score = 1.0 - (i * 0.1)  # ç¬¬ä¸€ä¸ªç»“æœ1.0ï¼Œç¬¬äºŒä¸ª0.9ï¼Œä»¥æ­¤ç±»æ¨
                
                results.append({
                    "rank": i + 1,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source", "Unknown"),
                    "score": score
                })
            
            return {
                "query": query,
                "results": results,
                "total": len(results)
            }
            
        except Exception as e:
            print(f"Error in search: {e}")
            return {
                "query": query,
                "results": [],
                "error": str(e)
            }
    
    def get_conversation_history(self, conversation_id: str) -> List[dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_histories.get(conversation_id, [])
    
    def clear_conversation_history(self, conversation_id: str):
        """æ¸…é™¤å¯¹è¯å†å²"""
        if conversation_id in self.conversation_histories:
            del self.conversation_histories[conversation_id]
            print(f"Cleared conversation history for {conversation_id}")
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'embedding_model': EMBEDDING_MODEL_NAME,
            'llm_model': QWEN_MODEL_NAME,  # ç›´æ¥ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
            'chunk_size': self.text_splitter._chunk_size,
            'chunk_overlap': self.text_splitter._chunk_overlap,
            'active_conversations': len(self.conversation_histories)
        } 