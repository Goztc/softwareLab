# RAGæµæ°´çº¿ - åŸºäºLangChainå®ç°
# Author: AI Assistant
# Version: 3.0 - åŸºäºRAG-Tongyi.pyä¼˜åŒ–

import os
from pathlib import Path
from typing import Optional, List

# LangChain imports - ä½¿ç”¨æœ€æ–°çš„å¯¼å…¥æ–¹å¼
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    TextLoader, UnstructuredMarkdownLoader, PyPDFLoader, Docx2txtLoader, DirectoryLoader
)
from langchain_core.documents import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatTongyi

# æœ¬åœ°å¯¼å…¥
from config import QWEN_API_KEY, QWEN_MODEL_NAME, EMBEDDING_MODEL_NAME, DOCUMENTS_ROOT_PATH, TEXT_SPLITTER_CONFIG

class RAGPipeline:
    """RAGæµæ°´çº¿ç±» - åŸºäºLangChainå®ç°ï¼Œå‚è€ƒRAG-Tongyi.pyä¼˜åŒ–"""
    
    def __init__(self):
        """åˆå§‹åŒ–RAGæµæ°´çº¿"""
        print("ğŸ”§ åˆå§‹åŒ–RAGæµæ°´çº¿...")
        
        # è®¾ç½®é€šä¹‰åƒé—®APIå¯†é’¥
        os.environ["DASHSCOPE_API_KEY"] = QWEN_API_KEY
        
        # åˆå§‹åŒ–é€šä¹‰åƒé—®æ¨¡å‹ - å‚è€ƒRAG-Tongyi.pyçš„å®ç°
        self.llm = ChatTongyi(
            model=QWEN_MODEL_NAME,  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
            temperature=0.1,  # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
            top_p=0.9,  # æ§åˆ¶è¯æ±‡é€‰æ‹©çš„å¤šæ ·æ€§
        )
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=TEXT_SPLITTER_CONFIG["chunk_size"],
            chunk_overlap=TEXT_SPLITTER_CONFIG["chunk_overlap"],
            separators=TEXT_SPLITTER_CONFIG["separators"]
        )
        print(f"ğŸ“‹ æ–‡æœ¬åˆ†å‰²å™¨é…ç½®: chunk_size={TEXT_SPLITTER_CONFIG['chunk_size']}, chunk_overlap={TEXT_SPLITTER_CONFIG['chunk_overlap']}")
        
        # åˆå§‹åŒ–æç¤ºæ¨¡æ¿ - å‚è€ƒRAG-Tongyi.pyçš„è®¾è®¡
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚è¯·ç¡®ä¿å›ç­”å‡†ç¡®ã€ç®€æ´ã€æœ‰å¸®åŠ©ã€‚
å¦‚æœæä¾›çš„æ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦‚å®è¯´æ˜ã€‚

ç›¸å…³æ–‡æ¡£:
{context}

é—®é¢˜: {question}

å›ç­”:"""
        )
        
        # åˆå§‹åŒ–ç¼“å­˜ç›¸å…³å±æ€§ - ä¼˜åŒ–æ€§èƒ½
        self.vectorstore_cache = {}  # å‘é‡å­˜å‚¨ç¼“å­˜ {path_key: vectorstore}
        self.retriever_cache = {}    # æ£€ç´¢å™¨ç¼“å­˜ {path_key: retriever}
        self.document_cache = {}     # æ–‡æ¡£ç¼“å­˜ {path_key: documents}
        self.cache_timestamps = {}   # ç¼“å­˜æ—¶é—´æˆ³ {path_key: timestamp}
        
        # æŒä¹…åŒ–å‘é‡å­˜å‚¨é…ç½®
        self.vector_store_root = Path("vector_stores")  # å‘é‡å­˜å‚¨æ ¹ç›®å½•
        self.vector_store_root.mkdir(exist_ok=True)     # ç¡®ä¿ç›®å½•å­˜åœ¨
        
        # æ–‡æ¡£æ ¹è·¯å¾„é…ç½®
        self.documents_root = Path(DOCUMENTS_ROOT_PATH)
        
        # åˆå§‹åŒ–å…¶ä»–å±æ€§
        self.conversation_histories = {}
        
        print("âœ… RAGæµæ°´çº¿åˆå§‹åŒ–æˆåŠŸï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ– + æŒä¹…åŒ–æ”¯æŒï¼‰")
    
    def _get_relative_path(self, absolute_path: str) -> str:
        """
        å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹äºæ–‡æ¡£æ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„
        
        Args:
            absolute_path: ç»å¯¹è·¯å¾„
            
        Returns:
            ç›¸å¯¹è·¯å¾„å­—ç¬¦ä¸²
        """
        try:
            abs_path = Path(absolute_path)
            # å¦‚æœè·¯å¾„åœ¨æ–‡æ¡£æ ¹ç›®å½•ä¸‹ï¼Œè¿”å›ç›¸å¯¹è·¯å¾„
            if abs_path.is_absolute() and self.documents_root in abs_path.parents or abs_path == self.documents_root:
                relative_path = abs_path.relative_to(self.documents_root)
                return str(relative_path).replace('\\', '/')  # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
            else:
                # å¦‚æœä¸åœ¨æ–‡æ¡£æ ¹ç›®å½•ä¸‹ï¼Œè¿”å›æ–‡ä»¶å
                return abs_path.name
        except (ValueError, OSError):
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸå§‹è·¯å¾„çš„æ–‡ä»¶åéƒ¨åˆ†
            return Path(absolute_path).name
    
    def _extract_sources_from_docs(self, docs: List[Document]) -> List[dict]:
        """
        ä»æ–‡æ¡£åˆ—è¡¨ä¸­æå–æºæ–‡æ¡£ä¿¡æ¯ï¼Œä¿ç•™æ‰€æœ‰ç›¸å…³ç‰‡æ®µå¹¶è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æºæ–‡æ¡£ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«contentå’Œsourceå­—æ®µ
        """
        sources = []
        source_file_counts = {}  # è®°å½•æ¯ä¸ªæ–‡ä»¶çš„ç‰‡æ®µæ•°é‡
        
        for doc in docs:
            source_info = doc.metadata.get("source", "Unknown")
            relative_source = self._get_relative_path(source_info)
            
            
            sources.append({
                "content": doc.page_content[:50],  # å¢åŠ å†…å®¹é¢„è§ˆé•¿åº¦
                "source": relative_source  # ä¿ç•™åŸå§‹æ–‡ä»¶è·¯å¾„
            })
        
        return sources
    
    def _load_documents_from_path(self, document_paths) -> List[Document]:
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ–‡æ¡£ï¼Œæ”¯æŒå¤šä¸ªè·¯å¾„"""
        # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(document_paths, str):
            document_paths = [document_paths]
        
        all_documents = []
        
        for document_path in document_paths:
            if not os.path.exists(document_path):
                print(f"âš ï¸  è­¦å‘Šï¼šæ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡: {document_path}")
                continue
            
            documents = []
            
            if os.path.isfile(document_path):
                # å•ä¸ªæ–‡ä»¶
                documents.extend(self._load_single_file(document_path))
                print(f"ğŸ“„ åŠ è½½æ–‡ä»¶: {document_path}")
            elif os.path.isdir(document_path):
                # ç›®å½• - é€’å½’éå†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹
                print(f"ğŸ“ åŠ è½½ç›®å½•: {document_path}")
                file_count = 0
                for file_path in Path(document_path).rglob("*"):
                    if file_path.is_file() and file_path.suffix.lower() in ['.txt', '.md', '.pdf', '.docx']:
                        file_documents = self._load_single_file(str(file_path))
                        documents.extend(file_documents)
                        if file_documents:  # åªæœ‰æˆåŠŸåŠ è½½çš„æ–‡ä»¶æ‰è®¡æ•°
                            file_count += 1
                            print(f"  ğŸ“„ å·²åŠ è½½: {file_path.name}")
                print(f"ğŸ“ ç›®å½• {document_path} å…±åŠ è½½ {file_count} ä¸ªæ–‡ä»¶")
            
            all_documents.extend(documents)
        
        print(f"ğŸ“š æ€»å…±åŠ è½½äº† {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        file_extension = Path(file_path).suffix.lower()
        
        try:
            if file_extension == '.txt':
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_extension == '.md':
                loader = UnstructuredMarkdownLoader(file_path)
            elif file_extension == '.pdf':
                loader = PyPDFLoader(file_path)
            elif file_extension in ['.docx', '.doc']:
                loader = Docx2txtLoader(file_path)
            else:
                # é»˜è®¤ä½œä¸ºæ–‡æœ¬æ–‡ä»¶å¤„ç†
                loader = TextLoader(file_path, encoding='utf-8')
            
            return loader.load()
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
            return []
    
    def _get_path_key(self, document_path) -> str:
        """ç”Ÿæˆæ–‡æ¡£è·¯å¾„çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        if isinstance(document_path, str):
            return document_path
        elif isinstance(document_path, list):
            return "|".join(sorted(document_path))
        else:
            return str(document_path)
    
    def _get_or_create_vectorstore(self, document_path) -> tuple:
        """
        è·å–æˆ–åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨ï¼ˆå¸¦ç¼“å­˜å’ŒæŒä¹…åŒ–ï¼‰
        
        Returns:
            tuple: (vectorstore, retriever, sources_info)
        """
        import time
        
        path_key = self._get_path_key(document_path)
        current_time = time.time()
        
        # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆï¼ˆç¼“å­˜æœ‰æ•ˆæœŸï¼š1å°æ—¶ï¼‰
        if (path_key in self.vectorstore_cache and 
            path_key in self.cache_timestamps and 
            current_time - self.cache_timestamps[path_key] < 3600):
            
            print(f"ğŸš€ ä½¿ç”¨å†…å­˜ç¼“å­˜çš„å‘é‡å­˜å‚¨: {path_key}")
            vectorstore = self.vectorstore_cache[path_key]
            retriever = self.retriever_cache[path_key]
            
            # é‡æ–°ç”Ÿæˆæºæ–‡æ¡£ä¿¡æ¯ï¼ˆè¿™éƒ¨åˆ†æ¯”è¾ƒè½»é‡ï¼‰
            documents = self.document_cache[path_key]
            sources_info = self._extract_sources_from_docs(documents)
            
            return vectorstore, retriever, sources_info
        
        # 2. å°è¯•ä»ç£ç›˜åŠ è½½æŒä¹…åŒ–çš„å‘é‡å­˜å‚¨
        persistent_path = self._get_persistent_path(document_path)
        if persistent_path.exists():
            print(f"ğŸ“‚ ä»ç£ç›˜åŠ è½½æŒä¹…åŒ–å‘é‡å­˜å‚¨: {path_key}")
            try:
                vectorstore = FAISS.load_local(
                    str(persistent_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                
                # æ›´æ–°å†…å­˜ç¼“å­˜
                retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
                self.vectorstore_cache[path_key] = vectorstore
                self.retriever_cache[path_key] = retriever
                self.cache_timestamps[path_key] = current_time
                
                # å°è¯•é‡æ–°åŠ è½½æ–‡æ¡£ä¿¡æ¯
                documents = self._load_documents_from_path(document_path)
                if documents:
                    self.document_cache[path_key] = documents
                    sources_info = self._extract_sources_from_docs(documents)
                    return vectorstore, retriever, sources_info
                
                print(f"âœ… ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨æˆåŠŸ: {path_key}")
                return vectorstore, retriever, []
                
            except Exception as e:
                print(f"âš ï¸  ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥ï¼Œå°†é‡æ–°æ„å»º: {e}")
        
        # 3. ç¼“å­˜ä¸å­˜åœ¨ä¸”ç£ç›˜æ²¡æœ‰ï¼Œé‡æ–°æ„å»º
        print(f"ğŸ”„ é‡æ–°æ„å»ºå‘é‡å­˜å‚¨: {path_key}")
        
        # åŠ è½½æ–‡æ¡£
        documents = self._load_documents_from_path(document_path)
        if not documents:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        print(f"ğŸ“Š å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºå‘é‡åŒ–")
        
        # åˆ†å‰²æ–‡æ¡£
        doc_splits = self.text_splitter.split_documents(documents)
        print(f"ğŸ”ª å·²å°†æ–‡æ¡£åˆ†å‰²ä¸º {len(doc_splits)} ä¸ªæ–‡æœ¬å—")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        vectorstore = FAISS.from_documents(doc_splits, self.embeddings)
        print("ğŸ—„ï¸  å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
        
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        
        # ç”Ÿæˆæºæ–‡æ¡£ä¿¡æ¯
        sources_info = self._extract_sources_from_docs(documents)
        
        # ç¼“å­˜ç»“æœ
        self.vectorstore_cache[path_key] = vectorstore
        self.retriever_cache[path_key] = retriever
        self.document_cache[path_key] = documents
        self.cache_timestamps[path_key] = current_time
        
        print(f"ğŸ’¾ å‘é‡å­˜å‚¨å·²ç¼“å­˜: {path_key}")
        
        return vectorstore, retriever, sources_info
    
    def clear_cache(self, document_path=None):
        """æ¸…é™¤ç¼“å­˜"""
        if document_path is None:
            # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            self.vectorstore_cache.clear()
            self.retriever_cache.clear()
            self.document_cache.clear()
            self.cache_timestamps.clear()
            print("ğŸ§¹ å·²æ¸…é™¤æ‰€æœ‰å‘é‡å­˜å‚¨ç¼“å­˜")
        else:
            # æ¸…é™¤ç‰¹å®šè·¯å¾„çš„ç¼“å­˜
            path_key = self._get_path_key(document_path)
            for cache_dict in [self.vectorstore_cache, self.retriever_cache, 
                             self.document_cache, self.cache_timestamps]:
                cache_dict.pop(path_key, None)
            print(f"ğŸ§¹ å·²æ¸…é™¤ç¼“å­˜: {path_key}")
    
    def query(self, question: str, document_path, top_k: int = 5) -> dict:
        """
        æŸ¥è¯¢æ–‡æ¡£å¹¶è¿”å›ç­”æ¡ˆï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document_path: æ–‡æ¡£è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€‚æ”¯æŒæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–æ··åˆï¼‰
            top_k: è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œæ¥æºæ–‡æ¡£çš„å­—å…¸
        """
        try:
            # ä½¿ç”¨ç¼“å­˜è·å–å‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨
            vectorstore, retriever, all_sources = self._get_or_create_vectorstore(document_path)
            
            # æ›´æ–°æ£€ç´¢å™¨çš„top_kå‚æ•°
            retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
            
            # åˆ›å»ºRetrievalQAé“¾ - å‚è€ƒRAG-Tongyi.pyçš„å®ç°
            retrieval_qa = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",  # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£æ”¾å…¥æç¤ºä¸­
                retriever=retriever,
                return_source_documents=True,  # åœ¨å“åº”ä¸­åŒ…å«æºæ–‡æ¡£
                chain_type_kwargs={"prompt": self.prompt_template}  # ä½¿ç”¨è‡ªå®šä¹‰æç¤º
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = retrieval_qa.invoke({"query": question})
            
            # è·å–æºæ–‡æ¡£å¹¶æ·»åŠ è°ƒè¯•ä¿¡æ¯
            source_documents = result.get("source_documents", [])
            print(f"ğŸ” æ£€ç´¢åˆ° {len(source_documents)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            
            # æå–æºæ–‡æ¡£ä¿¡æ¯
            sources = self._extract_sources_from_docs(source_documents)
            
            return {
                "question": question,
                "answer": result["result"],
                "sources": sources
            }
        
        except ValueError as e:
            print(f"âŒ æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "question": question,
                "answer": f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": []
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "question": question,
                "answer": f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": []
            }
    
    def chat(self, question: str, document_path, history: Optional[List] = None, conversation_id: str = "default", top_k: int = 5) -> dict:
        """
        å¸¦å†å²å¯¹è¯çš„æŸ¥è¯¢ - çœŸæ­£çš„å¯¹è¯å¼RAGå®ç°
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document_path: æ–‡æ¡£è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€‚æ”¯æŒæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–æ··åˆï¼‰
            history: å¯¹è¯å†å²
            conversation_id: å¯¹è¯IDï¼Œç”¨äºæ ‡è¯†ä¸åŒçš„å¯¹è¯ä¼šè¯
            top_k: è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œæ¥æºæ–‡æ¡£çš„å­—å…¸
        """
        try:
            # è·å–æˆ–åˆå§‹åŒ–å¯¹è¯å†å²
            if conversation_id not in self.conversation_histories:
                self.conversation_histories[conversation_id] = []
            
            current_history = self.conversation_histories[conversation_id]
            
            # å¦‚æœä¼ å…¥äº†historyå‚æ•°ï¼Œä½¿ç”¨å®ƒæ¥æ›´æ–°å½“å‰å†å²
            if history is not None:
                current_history = history.copy()
                self.conversation_histories[conversation_id] = current_history
            
            # ä½¿ç”¨ç¼“å­˜è·å–å‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨
            try:
                vectorstore, retriever, all_sources = self._get_or_create_vectorstore(document_path)
            except ValueError as e:
                return {
                    "question": question,
                    "answer": str(e),
                    "response": str(e),
                    "sources": [],
                    "updated_history": current_history,
                    "conversation_id": conversation_id
                }
            
            # ä½¿ç”¨æŒ‡å®šçš„top_kæ›´æ–°æ£€ç´¢å™¨å¹¶æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
            retrieved_docs = retriever.get_relevant_documents(question)
            
            print(f"ğŸ” æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
            
            # æ„å»ºåŒ…å«å†å²å¯¹è¯çš„æç¤º
            conversation_context = ""
            if current_history:
                print(f"ğŸ’¬ ä½¿ç”¨ {len(current_history)} è½®å†å²å¯¹è¯")
                conversation_context = "\nä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯å†å²:\n"
                for i, exchange in enumerate(current_history[-5:], 1):  # åªä½¿ç”¨æœ€è¿‘5è½®å¯¹è¯
                    conversation_context += f"ç¬¬{i}è½®å¯¹è¯:\n"
                    conversation_context += f"ç”¨æˆ·: {exchange.get('question', '')}\n"
                    conversation_context += f"åŠ©æ‰‹: {exchange.get('answer', '')}\n\n"
                conversation_context += "---\n"
            
            # åˆ›å»ºå¢å¼ºçš„æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«å†å²å¯¹è¯
            enhanced_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªåŸºäºæ–‡æ¡£çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{conversation_context}

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

å½“å‰ç”¨æˆ·é—®é¢˜: {question}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœé—®é¢˜æ¶‰åŠä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·ç»“åˆå†å²å¯¹è¯æ¥å›ç­”ã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦‚å®è¯´æ˜ã€‚

å›ç­”:"""
            
            # ç›´æ¥è°ƒç”¨LLMç”Ÿæˆå›ç­”
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå¸¦å†å²å¯¹è¯çš„å›ç­”...")
            response = self.llm.invoke(enhanced_prompt)
            
            # æå–å›ç­”å†…å®¹
            if hasattr(response, 'content'):
                answer = response.content
            else:
                answer = str(response)
            
            # æå–æºæ–‡æ¡£ä¿¡æ¯
            sources = self._extract_sources_from_docs(retrieved_docs)
            
            # æ·»åŠ å½“å‰å¯¹è¯åˆ°å†å²
            current_exchange = {
                "question": question,
                "answer": answer,
                "sources": sources,
                "timestamp": __import__('datetime').datetime.now().isoformat()
            }
            
            current_history.append(current_exchange)
            self.conversation_histories[conversation_id] = current_history
            
            print(f"âœ… å¯¹è¯å›ç­”ç”ŸæˆæˆåŠŸï¼Œå†å²è®°å½•å·²æ›´æ–° (å…±{len(current_history)}è½®)")
            
            return {
                "question": question,
                "answer": answer,
                "response": answer,  # ä¸ºäº†å…¼å®¹æ€§
                "sources": sources,
                "updated_history": current_history.copy(),
                "conversation_id": conversation_id
            }
            
        except Exception as e:
            print(f"âŒ å¯¹è¯æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "question": question,
                "answer": f"å¯¹è¯æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "response": f"å¯¹è¯æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": [],
                "updated_history": current_history,
                "conversation_id": conversation_id
            }
    
    def search(self, query: str, document_path, top_k: int = 5) -> dict:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            document_path: æ–‡æ¡£è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€‚æ”¯æŒæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–æ··åˆï¼‰
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        try:
            # ä½¿ç”¨ç¼“å­˜è·å–å‘é‡å­˜å‚¨
            vectorstore, _, all_sources = self._get_or_create_vectorstore(document_path)
            
            # æ‰§è¡Œæœç´¢
            retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
            docs = retriever.get_relevant_documents(query)
            
            results = []
            for i, doc in enumerate(docs):
                # å¦‚æœæ–‡æ¡£æœ‰scoreå±æ€§åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨åŸºäºæ’åçš„é»˜è®¤åˆ†æ•°
                score = getattr(doc, 'score', None)
                if score is None:
                    # åŸºäºæ’åç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿåˆ†æ•°ï¼Œæ’åè¶Šé å‰åˆ†æ•°è¶Šé«˜
                    score = 1.0 - (i * 0.1)  # ç¬¬ä¸€ä¸ªç»“æœ1.0ï¼Œç¬¬äºŒä¸ª0.9ï¼Œä»¥æ­¤ç±»æ¨
                
                results.append({
                    "rank": i + 1,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": self._get_relative_path(doc.metadata.get("source", "Unknown")),
                    "score": score
                })
            
            return {
                "query": query,
                "results": results,
                "total": len(results)
            }
            
        except ValueError as e:
            return {
                "query": query,
                "results": [],
                "error": str(e)
            }
        except Exception as e:
            print(f"Error in search: {e}")
            return {
                "query": query,
                "results": [],
                "error": str(e)
            }
    
    def get_conversation_history(self, conversation_id: str) -> List[dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_histories.get(conversation_id, [])
    
    def clear_conversation_history(self, conversation_id: str):
        """æ¸…é™¤å¯¹è¯å†å²"""
        if conversation_id in self.conversation_histories:
            del self.conversation_histories[conversation_id]
            print(f"Cleared conversation history for {conversation_id}")
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'embedding_model': EMBEDDING_MODEL_NAME,
            'llm_model': QWEN_MODEL_NAME,  # ç›´æ¥ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
            'chunk_size': self.text_splitter._chunk_size,
            'chunk_overlap': self.text_splitter._chunk_overlap,
            'active_conversations': len(self.conversation_histories),
            'cached_vectorstores': len(self.vectorstore_cache),
            'cache_keys': list(self.cache_timestamps.keys()),
            'persistent_vectorstores': self._list_persistent_vectorstores()
        }  
    
    def _list_persistent_vectorstores(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æŒä¹…åŒ–çš„å‘é‡å­˜å‚¨"""
        if not self.vector_store_root.exists():
            return []
        return [item.name for item in self.vector_store_root.iterdir() if item.is_dir()]
    
    def _get_persistent_path(self, document_path) -> Path:
        """è·å–æŒä¹…åŒ–å‘é‡å­˜å‚¨çš„è·¯å¾„"""
        path_key = self._get_path_key(document_path)
        safe_path_key = path_key.replace('/', '_').replace('\\', '_').replace('|', '_')
        return self.vector_store_root / safe_path_key
    
    def save_vectorstore(self, document_path, force_rebuild: bool = False) -> dict:
        """
        æ„å»ºå¹¶ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜
        
        Args:
            document_path: æ–‡æ¡£è·¯å¾„
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡æ–°æ„å»º
            
        Returns:
            ä¿å­˜ç»“æœä¿¡æ¯
        """
        try:
            path_key = self._get_path_key(document_path)
            persistent_path = self._get_persistent_path(document_path)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶é‡å»º
            if persistent_path.exists() and not force_rebuild:
                return {
                    "status": "exists",
                    "message": f"å‘é‡å­˜å‚¨å·²å­˜åœ¨: {path_key}",
                    "path": str(persistent_path)
                }
            
            print(f"ğŸ”¨ å¼€å§‹æ„å»ºå‘é‡å­˜å‚¨: {path_key}")
            
            # åŠ è½½æ–‡æ¡£
            documents = self._load_documents_from_path(document_path)
            if not documents:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
            print(f"ğŸ“Š å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºå‘é‡åŒ–")
            
            # åˆ†å‰²æ–‡æ¡£
            doc_splits = self.text_splitter.split_documents(documents)
            print(f"ğŸ”ª å·²å°†æ–‡æ¡£åˆ†å‰²ä¸º {len(doc_splits)} ä¸ªæ–‡æœ¬å—")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vectorstore = FAISS.from_documents(doc_splits, self.embeddings)
            print("ğŸ—„ï¸  å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
            
            # ä¿å­˜åˆ°ç£ç›˜
            vectorstore.save_local(str(persistent_path))
            print(f"ğŸ’¾ å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {persistent_path}")
            
            # åŒæ—¶æ›´æ–°å†…å­˜ç¼“å­˜
            import time
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            self.vectorstore_cache[path_key] = vectorstore
            self.retriever_cache[path_key] = retriever
            self.document_cache[path_key] = documents
            self.cache_timestamps[path_key] = time.time()
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "document_path": document_path,
                "document_count": len(documents),
                "chunk_count": len(doc_splits),
                "created_at": time.time(),
                "embedding_model": EMBEDDING_MODEL_NAME
            }
            
            import json
            metadata_path = persistent_path / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            return {
                "status": "created",
                "message": f"å‘é‡å­˜å‚¨æ„å»ºå¹¶ä¿å­˜æˆåŠŸ: {path_key}",
                "path": str(persistent_path),
                "metadata": metadata
            }
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}"
            }
    
    def load_vectorstore(self, document_path) -> dict:
        """
        ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨
        
        Args:
            document_path: æ–‡æ¡£è·¯å¾„
            
        Returns:
            åŠ è½½ç»“æœä¿¡æ¯
        """
        try:
            path_key = self._get_path_key(document_path)
            persistent_path = self._get_persistent_path(document_path)
            
            if not persistent_path.exists():
                return {
                    "status": "not_found",
                    "message": f"å‘é‡å­˜å‚¨ä¸å­˜åœ¨: {path_key}"
                }
            
            print(f"ğŸ“‚ ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨: {path_key}")
            
            # ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨
            vectorstore = FAISS.load_local(
                str(persistent_path), 
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_path = persistent_path / "metadata.json"
            metadata = {}
            if metadata_path.exists():
                import json
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            import time
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            self.vectorstore_cache[path_key] = vectorstore
            self.retriever_cache[path_key] = retriever
            self.cache_timestamps[path_key] = time.time()
            
            # å¦‚æœæœ‰å…ƒæ•°æ®ä¸­çš„æ–‡æ¡£ä¿¡æ¯ï¼Œå¯ä»¥é‡æ–°åŠ è½½æ–‡æ¡£ï¼ˆå¯é€‰ï¼‰
            if "document_path" in metadata:
                try:
                    documents = self._load_documents_from_path(metadata["document_path"])
                    self.document_cache[path_key] = documents
                except:
                    pass  # å¦‚æœæ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œä¸å½±å“å‘é‡å­˜å‚¨çš„ä½¿ç”¨
            
            print(f"âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ: {path_key}")
            
            return {
                "status": "loaded",
                "message": f"å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ: {path_key}",
                "path": str(persistent_path),
                "metadata": metadata
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}"
            }
    
    def list_vectorstores(self) -> dict:
        """
        åˆ—å‡ºæ‰€æœ‰æŒä¹…åŒ–çš„å‘é‡å­˜å‚¨
        
        Returns:
            å‘é‡å­˜å‚¨åˆ—è¡¨ä¿¡æ¯
        """
        try:
            vectorstores = []
            
            if self.vector_store_root.exists():
                for item in self.vector_store_root.iterdir():
                    if item.is_dir():
                        metadata_path = item / "metadata.json"
                        metadata = {}
                        if metadata_path.exists():
                            import json
                            try:
                                with open(metadata_path, 'r', encoding='utf-8') as f:
                                    metadata = json.load(f)
                            except:
                                pass
                        
                        vectorstores.append({
                            "name": item.name,
                            "path": str(item),
                            "metadata": metadata
                        })
            
            return {
                "status": "success",
                "vectorstores": vectorstores,
                "total": len(vectorstores)
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"åˆ—å‡ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}"
            }